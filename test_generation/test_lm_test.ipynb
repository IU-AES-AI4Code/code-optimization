{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0,1'\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import subprocess\n",
    "\n",
    "def read_method_code(path) -> str: \n",
    "    # for future, code will be modified and further its values\n",
    "    # will be used for the form_prompt_for_method\n",
    "    pass\n",
    "\n",
    "def initialize_models():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-7b-base-v1.5\", trust_remote_code=True, device_map = \"auto\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-7b-base-v1.5\", trust_remote_code=True, device_map = \"auto\")\n",
    "    return tokenizer, model\n",
    "\n",
    "def form_prompt_for_method(method_code: str) -> str:\n",
    "    prefix = \"import pytest\"\n",
    "    test_comment = \"# test for the method above\\n# those tests cover each possible branch just once, no excessive repeats\"\n",
    "    prompt = \"\\n\\n\".join([prefix, method_code, test_comment])\n",
    "    return prompt\n",
    "    \n",
    "def generate_code_tests(method_code: str, tokenizer: AutoTokenizer, model: AutoModelForCausalLM) -> str:\n",
    "    prompt = form_prompt_for_method(method_code)\n",
    "    tokenized_prompt = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    tokenized_output = model.generate(**tokenized_prompt, max_new_tokens=200, do_sample = False, use_cache = True)\n",
    "    whole_code = tokenizer.decode(tokenized_output[0], skip_special_tokens=True)\n",
    "    return whole_code\n",
    "\n",
    "def fix_interrupted_gen(whole_code: str) -> str:\n",
    "    function_split = whole_code.split(\"def\")\n",
    "    function_split_count = len(function_split)\n",
    "    # if there are more than two functions the last one could be interrupted in the middle\n",
    "    # we want to get rid of such a function so that code is interpretable\n",
    "    # if there are just two functions last line could be interrupted\n",
    "    if function_split_count > 3:\n",
    "        valid_parts = function_split[:-1]\n",
    "        working_code = 'def'.join(valid_parts)\n",
    "    else:\n",
    "        line_split = whole_code.split(\"\\n\")\n",
    "        valid_lines = line_split[:-1]\n",
    "        working_code = '\\n'.join(valid_lines)\n",
    "    \n",
    "    return working_code\n",
    "\n",
    "def write_code_tests(code: str, path: str = \"code_test.py\") -> None:\n",
    "    with open(path, 'w') as file:\n",
    "        file.write(code)\n",
    "\n",
    "def get_total_file_coverage(path: str = \"code_test.py\") -> int:\n",
    "    test_command = f\"pytest {path} --cov={path.split('.')[0]}\"\n",
    "    test_result = subprocess.run(test_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True, text=True)\n",
    "    out = test_result.stdout\n",
    "\n",
    "    # identify total result line\n",
    "    linesplit = out.split('\\n')\n",
    "    for line in linesplit:\n",
    "        if line[:5] == 'TOTAL':\n",
    "            total_line = line\n",
    "            break\n",
    "    \n",
    "    # retrieve just the coverage info\n",
    "    coverage_string = total_line.split()[-1]\n",
    "    coverage_percent = int(coverage_string[:-1])\n",
    "    return coverage_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.70s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer, model = initialize_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1477: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test coverage is 100%\n"
     ]
    }
   ],
   "source": [
    "whole_code = generate_code_tests(\"\"\"def max_of_three(a, b, c):\n",
    "    \\\"\\\"\\\"\n",
    "    This function returns the maximum of three numbers.\n",
    "    \\\"\\\"\\\"\n",
    "    if a >= b and a >= c:\n",
    "        return a\n",
    "    elif b >= a and b >= c:\n",
    "        return b\n",
    "    else:\n",
    "        return c\n",
    "\n",
    "\"\"\", tokenizer, model)\n",
    "\n",
    "working_code = fix_interrupted_gen(whole_code)\n",
    "write_code_tests(working_code)\n",
    "test_coverage = get_total_file_coverage()\n",
    "\n",
    "print(f\"Test coverage is {test_coverage}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr 17 16:19:32 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.161.07             Driver Version: 535.161.07   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          Off | 00000000:22:00.0 Off |                    0 |\n",
      "| N/A   34C    P0              67W / 300W |  23440MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100 80GB PCIe          Off | 00000000:41:00.0 Off |                    0 |\n",
      "| N/A   35C    P0              60W / 300W |  24130MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100 80GB PCIe          Off | 00000000:A1:00.0 Off |                    0 |\n",
      "| N/A   38C    P0              65W / 300W |  48078MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100 80GB PCIe          Off | 00000000:E1:00.0 Off |                    0 |\n",
      "| N/A   41C    P0              68W / 300W |  80820MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test FastAPI application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tests': 'def test_max_of_three():\\n# test for the method above\\n# those tests cover each possible branch just once, no excessive repeats', 'coverage': 100}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Define the code for which you want to generate tests and compute coverage\n",
    "code = \"\"\"\n",
    "def max_of_three(a, b, c):\n",
    "    \\\"\\\"\\\"\n",
    "    This function returns the maximum of three numbers.\n",
    "    \\\"\\\"\\\"\n",
    "    if a >= b and a >= c:\n",
    "        return a\n",
    "    elif b >= a and b >= c:\n",
    "        return b\n",
    "    else:\n",
    "        return c\n",
    "\"\"\"\n",
    "\n",
    "# Create a dictionary with the code and convert it to JSON\n",
    "data = {\"code\": code}\n",
    "json_data = json.dumps(data)\n",
    "\n",
    "# Send a POST request to generate tests and compute coverage\n",
    "response = requests.post(\"http://localhost:8001/gen_test\", data=json_data, headers={'Content-Type': 'application/json'})\n",
    "\n",
    "# Print the response\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests, coverage = response.json()['tests'], response.json()['coverage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'coverage': 75}\n"
     ]
    }
   ],
   "source": [
    "data = {\"code\": code, \"tests\": tests}\n",
    "json_data = json.dumps(data)\n",
    "\n",
    "# Send a POST request to generate tests and compute coverage\n",
    "new_response = requests.post(\"http://localhost:8001/compute_coverage\", data=json_data, headers={'Content-Type': 'application/json'})\n",
    "\n",
    "# Print the response\n",
    "print(new_response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
