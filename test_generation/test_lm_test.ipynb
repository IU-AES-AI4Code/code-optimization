{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import subprocess\n",
    "\n",
    "def read_method_code(path) -> str: \n",
    "    # for future, code will be modified and further its values\n",
    "    # will be used for the form_prompt_for_method\n",
    "    pass\n",
    "\n",
    "def initialize_models():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-7b-base-v1.5\", trust_remote_code=True, device_map = \"auto\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-7b-base-v1.5\", trust_remote_code=True, device_map = \"auto\")\n",
    "    return tokenizer, model\n",
    "\n",
    "def form_prompt_for_method(method_code: str) -> str:\n",
    "    prefix = \"import pytest\"\n",
    "    test_comment = \"# test for the method above\\n# those tests cover each possible branch just once, no excessive repeats\"\n",
    "    prompt = \"\\n\\n\".join([prefix, method_code, test_comment])\n",
    "    return prompt\n",
    "    \n",
    "def generate_code_tests(method_code: str, tokenizer: AutoTokenizer, model: AutoModelForCausalLM) -> str:\n",
    "    prompt = form_prompt_for_method(method_code)\n",
    "    tokenized_prompt = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    tokenized_output = model.generate(**tokenized_prompt, max_new_tokens=200, do_sample = False, use_cache = True)\n",
    "    whole_code = tokenizer.decode(tokenized_output[0], skip_special_tokens=True)\n",
    "    return whole_code\n",
    "\n",
    "def fix_interrupted_gen(whole_code: str) -> str:\n",
    "    function_split = whole_code.split(\"def\")\n",
    "    function_split_count = len(function_split)\n",
    "    # if there are more than two functions the last one could be interrupted in the middle\n",
    "    # we want to get rid of such a function so that code is interpretable\n",
    "    # if there are just two functions last line could be interrupted\n",
    "    if function_split_count > 3:\n",
    "        valid_parts = function_split[:-1]\n",
    "        working_code = 'def'.join(valid_parts)\n",
    "    else:\n",
    "        line_split = whole_code.split(\"\\n\")\n",
    "        valid_lines = line_split[:-1]\n",
    "        working_code = '\\n'.join(valid_lines)\n",
    "    \n",
    "    return working_code\n",
    "\n",
    "def write_code_tests(code: str, path: str = \"code_test.py\") -> None:\n",
    "    with open(path, 'w') as file:\n",
    "        file.write(code)\n",
    "\n",
    "def get_total_file_coverage(path: str = \"code_test.py\") -> int:\n",
    "    test_command = f\"pytest {path} --cov={path.split('.')[0]}\"\n",
    "    test_result = subprocess.run(test_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True, text=True)\n",
    "    out = test_result.stdout\n",
    "\n",
    "    # identify total result line\n",
    "    linesplit = out.split('\\n')\n",
    "    for line in linesplit:\n",
    "        if line[:5] == 'TOTAL':\n",
    "            total_line = line\n",
    "            break\n",
    "    \n",
    "    # retrieve just the coverage info\n",
    "    coverage_string = total_line.split()[-1]\n",
    "    coverage_percent = int(coverage_string[:-1])\n",
    "    return coverage_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:08<00:00,  2.70s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer, model = initialize_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1477: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test coverage is 100%\n"
     ]
    }
   ],
   "source": [
    "whole_code = generate_code_tests(\"\"\"def max_of_three(a, b, c):\n",
    "    \\\"\\\"\\\"\n",
    "    This function returns the maximum of three numbers.\n",
    "    \\\"\\\"\\\"\n",
    "    if a >= b and a >= c:\n",
    "        return a\n",
    "    elif b >= a and b >= c:\n",
    "        return b\n",
    "    else:\n",
    "        return c\n",
    "\n",
    "\"\"\", tokenizer, model)\n",
    "\n",
    "working_code = fix_interrupted_gen(whole_code)\n",
    "write_code_tests(working_code)\n",
    "test_coverage = get_total_file_coverage()\n",
    "\n",
    "print(f\"Test coverage is {test_coverage}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
