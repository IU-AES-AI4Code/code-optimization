import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

def stochastic_gradient_descent(X, y, epochs=100, lr=0.01):
    m, n = X.shape
    weights = np.random.rand(n)
    bias = 0
    for _ in range(epochs):
        for i in range(m):
            x = X[i]
            y_pred = sigmoid(np.dot(weights, x) + bias)
            error = y[i] - y_pred
            weights += lr * error * y_pred * (1 - y_pred) * x
            bias += lr * error * y_pred * (1 - y_pred)
    return weights, bias