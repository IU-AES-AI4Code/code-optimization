import numpy as np

def q_learning(env, num_episodes, alpha, gamma, epsilon):
    # Initialize table with zeros
    q_table = np.zeros([env.observation_space.n, env.action_space.n])
    
    for i in range(num_episodes):
        state = env.reset()
        done = False
        
        while not done:
            # Choose action a in current world state
            if np.random.uniform(0, 1) < epsilon:
                action = env.action_space.sample() # Explore action space
            else:
                action = np.argmax(q_table[state, :]) # Exploit learned values
            
            # Take action a, observe new state and reward
            new_state, reward, done, info = env.step(action)
            
            # Update Q(s, a)
            q_table[state, action] = q_table[state, action] + alpha * (reward + gamma * np.max(q_table[new_state, :]) - q_table[state, action])
            
            state = new_state
            
    return q_table